"use strict";(self.webpackChunkdocusaurus_resource=self.webpackChunkdocusaurus_resource||[]).push([[1996],{28453:(e,a,n)=>{n.d(a,{R:()=>i,x:()=>s});var o=n(96540);const r={},l=o.createContext(r);function i(e){const a=o.useContext(l);return o.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function s(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),o.createElement(l.Provider,{value:a},e.children)}},38935:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>t,contentTitle:()=>s,default:()=>u,frontMatter:()=>i,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"genai/all-about-models/model-providers/local/docker-model-runner","title":"Docker Model Runner","description":"You love Java? You\'re already using Docker? And you\'re exploring AI? Then this one is for you!","source":"@site/docs/10-genai/20-all-about-models/40-model-providers/20-local/30-docker-model-runner.md","sourceDirName":"10-genai/20-all-about-models/40-model-providers/20-local","slug":"/genai/all-about-models/model-providers/local/docker-model-runner","permalink":"/java-ai-resource/docs/genai/all-about-models/model-providers/local/docker-model-runner","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/10-genai/20-all-about-models/40-model-providers/20-local/30-docker-model-runner.md","tags":[],"version":"current","sidebarPosition":30,"frontMatter":{"title":"Docker Model Runner","sidebar_position":30,"hide_title":true,"level":"advanced","status":"published","visibility":"public","programming-language":"Java"},"sidebar":"tutorialSidebar","previous":{"title":"Ollama","permalink":"/java-ai-resource/docs/genai/all-about-models/model-providers/local/ollama"},"next":{"title":"LM Studio","permalink":"/java-ai-resource/docs/genai/all-about-models/model-providers/local/lm-studio"}}');var r=n(74848),l=n(28453);const i={title:"Docker Model Runner",sidebar_position:30,hide_title:!0,level:"advanced",status:"published",visibility:"public","programming-language":"Java"},s="Docker Model Runner",t={},d=[{value:"Why is this cool",id:"why-is-this-cool",level:2},{value:"Try it in 5 minutes",id:"try-it-in-5-minutes",level:2},{value:"What models are available?",id:"what-models-are-available",level:2},{value:"What&#39;s next?",id:"whats-next",level:2}];function c(e){const a={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",hr:"hr",p:"p",pre:"pre",strong:"strong",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.header,{children:(0,r.jsx)(a.h1,{id:"docker-model-runner",children:"Docker Model Runner"})}),"\n",(0,r.jsx)(a.p,{children:"You love Java? You're already using Docker? And you're exploring AI? Then this one is for you!"}),"\n",(0,r.jsx)(a.p,{children:"Docker launched Docker Model Runner (DMR) in early 2025: it's a model runtime engine that ships with Docker Desktop and model weights that you can pull straight from Docker Hub, just like common images. Think DeepSeek, Gemma3, Phi4, \u2026"}),"\n",(0,r.jsx)(a.p,{children:"DMR makes running language models locally super easy, including GPU-acceleration (for faster inference), OpenAI-compatible APIs, and fully wired into your existing developer setup. You can use it in Java with LangChain4j or SpringAI."}),"\n",(0,r.jsx)(a.h2,{id:"why-is-this-cool",children:"Why is this cool"}),"\n",(0,r.jsxs)(a.p,{children:["Docker just turned running LLMs into something as easy as ",(0,r.jsx)(a.code,{children:"docker model run"}),". You don't need to understand model architecture or inference backends, you just:"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"docker model pull ai/gemma3\ndocker model run ai/gemma3\n"})}),"\n",(0,r.jsxs)(a.p,{children:["That's it. It's the kind of developer experience we've been missing in the AI tooling world. Fast, familiar, and frictionless. Especially for teams who already use Docker in their stack. They're curating and hosting popular models like ",(0,r.jsx)(a.strong,{children:"Gemma"}),", ",(0,r.jsx)(a.strong,{children:"LLaMA"}),", and ",(0,r.jsx)(a.strong,{children:"DeepSeek"})," directly on ",(0,r.jsx)(a.a,{href:"http://hub.docker.com/u/ai",children:"Docker Hub"}),", so you don't have to source weights or worry about sketchy downloads."]}),"\n",(0,r.jsx)(a.p,{children:"It's fast. It's GPU-accelerated. And it plugs directly into your Java app."}),"\n",(0,r.jsx)(a.h2,{id:"try-it-in-5-minutes",children:"Try it in 5 minutes"}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.strong,{children:"Note:"})," At the moment only available on Mac (Windows will within a month or so)."]}),"\n",(0,r.jsx)(a.p,{children:"First, let's get your Docker Desktop ready for the Model Runner. Make sure to be on a recent Docker version (4.40+ required). In settings, scroll down to Features in Development, and check:"}),"\n",(0,r.jsx)("img",{src:"/java-ai-resource/img/10-genai/20-all-about-models/enable_dmr.webp",alt:"Enable Docker Model Runner settings",style:{width:"70%",display:"block",margin:"auto"}}),"\n",(0,r.jsx)(a.p,{children:"The first check enables the Model Runner in CLI, the second one makes\nsure we can access it with LangChain4j. Click Apply and Restart."}),"\n",(0,r.jsx)(a.p,{children:"List the available commands and check if the installation was successful by opening a terminal:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"docker model command\n"})}),"\n",(0,r.jsx)(a.p,{children:"Now let's pull and run our first model:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"docker model pull ai/gemma3:latest\ndocker model run ai/gemma3\n"})}),"\n",(0,r.jsxs)(a.p,{children:["To use this model directly from Java, I put together a minimal, no-frameworks chatbot with memory using LangChain4j: \ud83d\udc49 ",(0,r.jsx)(a.a,{href:"https://github.com/LizeRaes/docker-model-runner-langchain4j",children:"github.com/LizeRaes/docker-model-runner-langchain4j"})]}),"\n",(0,r.jsx)("img",{src:"/java-ai-resource/img/10-genai/20-all-about-models/dmr_chat.webp",alt:"Docker Model Runner Chatbot",style:{width:"70%",display:"block",margin:"auto"}}),"\n",(0,r.jsx)(a.p,{children:"Follow the instructions in the readme, and there you go, you're now chatting with a local model via Java."}),"\n",(0,r.jsxs)(a.p,{children:["The magic happens in ",(0,r.jsx)(a.code,{children:"ChatServer.java"}),":"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-java",children:'ChatLanguageModel model = OpenAiChatModel.builder()\n    .apiKey("not needed")\n    .baseUrl("http://localhost:12434/engines/llama.cpp/v1")\n    .modelName("ai/gemma3")\n    .build();\n'})}),"\n",(0,r.jsx)(a.h2,{id:"what-models-are-available",children:"What models are available?"}),"\n",(0,r.jsxs)(a.p,{children:["Browse ",(0,r.jsx)(a.a,{href:"https://hub.docker.com/u/ai",children:"Docker's AI Hub"})," to see what's already there. More models are added regularly."]}),"\n",(0,r.jsx)(a.p,{children:"To check file size and resources, just open a model page on Docker Hub \u2014 you'll see the size, architecture, and version. For example, for Gemma3"}),"\n",(0,r.jsx)("img",{src:"/java-ai-resource/img/10-genai/20-all-about-models/gemma3_size.webp",alt:"Gemma3 Model Size",style:{width:"70%",display:"block",margin:"auto"}}),"\n",(0,r.jsxs)(a.p,{children:["Once you have the model running you can check your GPU usage via (MacOS):\n",(0,r.jsx)(a.strong,{children:"Activity Monitor \u2192 Window \u2192 GPU History"})]}),"\n",(0,r.jsx)("img",{src:"/java-ai-resource/img/10-genai/20-all-about-models/dmr_gpu.webp",alt:"Docker Model Runner GPU Usage",style:{width:"70%",display:"block",margin:"auto"}}),"\n",(0,r.jsx)(a.h2,{id:"whats-next",children:"What's next?"}),"\n",(0,r.jsx)(a.p,{children:"Docker's R&D team works on a very ambitious AI roadmap. DMR is available for Mac and Windows. They already offer containerized MCP servers and are on their way to make all aspects of AI model inference a breeze. Docker Offload lets you run all that on a cloud GPU in one click, and Agent Compose let's you define models and agents"}),"\n",(0,r.jsx)(a.p,{children:"Your feedback is super valuable for making everything work smoothly, so if you encounter a glitch or miss an essential feature, let the Docker team know via:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"docker feedback\n"})}),"\n",(0,r.jsx)(a.p,{children:"Have fun building with it!"}),"\n",(0,r.jsx)(a.hr,{}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.em,{children:"Written by Lize Raes"})})]})}function u(e={}){const{wrapper:a}={...(0,l.R)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);