"use strict";(self.webpackChunkdocusaurus_resource=self.webpackChunkdocusaurus_resource||[]).push([[5343],{28453:(e,o,n)=>{n.d(o,{R:()=>i,x:()=>a});var l=n(96540);const s={},r=l.createContext(s);function i(e){const o=l.useContext(r);return l.useMemo(function(){return"function"==typeof e?e(o):{...o,...e}},[o,e])}function a(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),l.createElement(r.Provider,{value:o},e.children)}},44716:(e,o,n)=>{n.r(o),n.d(o,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>l,toc:()=>t});const l=JSON.parse('{"id":"genai/all-about-models/model-providers/local/index","title":"Local Models","description":"Quick Navigation","source":"@site/docs/10-genai/20-all-about-models/40-model-providers/20-local/index.md","sourceDirName":"10-genai/20-all-about-models/40-model-providers/20-local","slug":"/genai/all-about-models/model-providers/local/","permalink":"/java-ai-resource/docs/genai/all-about-models/model-providers/local/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/10-genai/20-all-about-models/40-model-providers/20-local/index.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"title":"Local Models","sidebar_position":10,"hide_title":true,"level":"intermediate","status":"published","visibility":"public"},"sidebar":"tutorialSidebar","previous":{"title":"Gemini","permalink":"/java-ai-resource/docs/genai/all-about-models/model-providers/commercial/gemini"},"next":{"title":"Huggingface","permalink":"/java-ai-resource/docs/genai/all-about-models/model-providers/local/huggingface"}}');var s=n(74848),r=n(28453);const i={title:"Local Models",sidebar_position:10,hide_title:!0,level:"intermediate",status:"published",visibility:"public"},a="Local Models",d={},t=[{value:"Quick Navigation",id:"quick-navigation",level:2},{value:"\ud83c\udfaf The Advantages",id:"-the-advantages",level:2},{value:"\u26a0\ufe0f The Disadvantages",id:"\ufe0f-the-disadvantages",level:2},{value:"How to Choose the Right Local Model",id:"how-to-choose-the-right-local-model",level:2},{value:"How to Run Local Models",id:"how-to-run-local-models",level:2}];function c(e){const o={a:"a",em:"em",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(o.header,{children:(0,s.jsx)(o.h1,{id:"local-models",children:"Local Models"})}),"\n",(0,s.jsx)(o.h2,{id:"quick-navigation",children:"Quick Navigation"}),"\n",(0,s.jsxs)(o.ul,{children:["\n",(0,s.jsx)(o.li,{children:(0,s.jsx)(o.a,{href:"#-the-advantages",children:"The Advantages"})}),"\n",(0,s.jsx)(o.li,{children:(0,s.jsx)(o.a,{href:"#%EF%B8%8F-the-disadvantages",children:"The Disadvantages"})}),"\n",(0,s.jsx)(o.li,{children:(0,s.jsx)(o.a,{href:"#how-to-choose-the-right-local-model",children:"How to Choose the Right Local Model"})}),"\n",(0,s.jsx)(o.li,{children:(0,s.jsx)(o.a,{href:"#how-to-run-local-models",children:"How to Run Local Models"})}),"\n"]}),"\n",(0,s.jsx)(o.hr,{}),"\n",(0,s.jsx)(o.p,{children:"You can run LLMs and other AI models locally, on your own computer or specialized hardware in your home or company (on-prem)."}),"\n",(0,s.jsxs)(o.p,{children:["You will typically run an ",(0,s.jsx)(o.strong,{children:"Open Source Model"}),", where you can download the weights and some runtime to actually run your model."]}),"\n",(0,s.jsx)(o.h2,{id:"-the-advantages",children:"\ud83c\udfaf The Advantages"}),"\n",(0,s.jsxs)(o.ul,{children:["\n",(0,s.jsxs)(o.li,{children:[(0,s.jsx)(o.strong,{children:"They work offline"})," - No internet connection required once the model is downloaded"]}),"\n",(0,s.jsxs)(o.li,{children:[(0,s.jsx)(o.strong,{children:"Your data stays private"})," - Any questions or documents you send to the model do not leave your system"]}),"\n",(0,s.jsxs)(o.li,{children:[(0,s.jsx)(o.strong,{children:"No SaaS model usage costs"})," - No API key or per-request charges"]}),"\n",(0,s.jsxs)(o.li,{children:[(0,s.jsx)(o.strong,{children:"With the right setup, local models can be very fast"})," - No network latency, direct GPU access"]}),"\n"]}),"\n",(0,s.jsx)(o.h2,{id:"\ufe0f-the-disadvantages",children:"\u26a0\ufe0f The Disadvantages"}),"\n",(0,s.jsxs)(o.ul,{children:["\n",(0,s.jsxs)(o.li,{children:[(0,s.jsx)(o.strong,{children:"On your laptop you can only run models that are an order of magnitude smaller"})," than the SaaS models, depending on your hardware"]}),"\n",(0,s.jsxs)(o.li,{children:[(0,s.jsx)(o.strong,{children:"Hardware to run big models (50GB+) is expensive"})," - Requires high-end GPUs and lots of RAM"]}),"\n",(0,s.jsxs)(o.li,{children:[(0,s.jsx)(o.strong,{children:"Model switching is inefficient"})," - The weights need to be loaded in memory each time you use another model (gigabytes)"]}),"\n",(0,s.jsxs)(o.li,{children:[(0,s.jsx)(o.strong,{children:"Smaller models mean notably worse quality"})," - Issues with complex tasks and tool calling, etc."]}),"\n",(0,s.jsxs)(o.li,{children:[(0,s.jsx)(o.strong,{children:"Use up a lot of memory"})," from your local machine"]}),"\n",(0,s.jsxs)(o.li,{children:[(0,s.jsx)(o.strong,{children:"They don't scale well for large-scale deployments"})," - Concurrency issues when different users try to simultaneously use the model, unless you set up a kubernetes cluster to manage availability and throughput"]}),"\n"]}),"\n",(0,s.jsx)(o.h2,{id:"how-to-choose-the-right-local-model",children:"How to Choose the Right Local Model"}),"\n",(0,s.jsx)(o.p,{children:"The best model for your use case depends on factors like:"}),"\n",(0,s.jsxs)(o.ul,{children:["\n",(0,s.jsxs)(o.li,{children:[(0,s.jsx)(o.strong,{children:"Available memory"})," on the local machine"]}),"\n",(0,s.jsxs)(o.li,{children:[(0,s.jsx)(o.strong,{children:"Required speed"})," for your application"]}),"\n",(0,s.jsxs)(o.li,{children:[(0,s.jsx)(o.strong,{children:"Required quality"})," for your specific tasks"]}),"\n",(0,s.jsxs)(o.li,{children:[(0,s.jsx)(o.strong,{children:"Model specialization"})," (coding, reasoning, etc.)"]}),"\n",(0,s.jsx)(o.li,{children:(0,s.jsx)(o.strong,{children:"Required context window size"})}),"\n"]}),"\n",(0,s.jsxs)(o.p,{children:["Models like ",(0,s.jsx)(o.strong,{children:"Qwen3"})," are said to perform better for coding and tool calling. A model like ",(0,s.jsx)(o.strong,{children:"DeepSeek"})," does reasoning to tackle complex tasks better."]}),"\n",(0,s.jsxs)(o.p,{children:["You can check performance in leader boards or benchmarks like ",(0,s.jsx)(o.a,{href:"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/",children:"HuggingFace's Open LLM Leaderboard"})," (with categories like 'For Edge Devices' and 'For the GPU Rich', ...) and its ",(0,s.jsx)(o.a,{href:"https://huggingface.co/spaces/open-llm-leaderboard/comparator",children:"Open LLM Comparator"}),"."]}),"\n",(0,s.jsx)(o.h2,{id:"how-to-run-local-models",children:"How to Run Local Models"}),"\n",(0,s.jsx)(o.p,{children:"Here are the most popular ways to run local models:"}),"\n",(0,s.jsxs)(o.ul,{children:["\n",(0,s.jsxs)(o.li,{children:["\n",(0,s.jsxs)(o.p,{children:[(0,s.jsx)(o.strong,{children:(0,s.jsx)(o.a,{href:"/java-ai-resource/docs/genai/all-about-models/model-providers/local/huggingface",children:"\ud83e\udd17 HuggingFace"})})," - Easy model hosting and inference. Use the HuggingFace ecosystem to download and run models with their optimized libraries."]}),"\n"]}),"\n",(0,s.jsxs)(o.li,{children:["\n",(0,s.jsxs)(o.p,{children:[(0,s.jsx)(o.strong,{children:(0,s.jsx)(o.a,{href:"/java-ai-resource/docs/genai/all-about-models/model-providers/local/ollama",children:"\ud83e\udd99 Ollama"})})," - Simple local model management. Pull and run models with a single command, perfect for quick experimentation."]}),"\n"]}),"\n",(0,s.jsxs)(o.li,{children:["\n",(0,s.jsxs)(o.p,{children:[(0,s.jsx)(o.strong,{children:(0,s.jsx)(o.a,{href:"/java-ai-resource/docs/genai/all-about-models/model-providers/local/docker-model-runner",children:"\ud83d\udc33 Docker Model Runner"})})," - Containerized model inference. Run models in Docker containers with GPU acceleration and OpenAI-compatible APIs."]}),"\n"]}),"\n",(0,s.jsxs)(o.li,{children:["\n",(0,s.jsxs)(o.p,{children:[(0,s.jsx)(o.strong,{children:(0,s.jsx)(o.a,{href:"/java-ai-resource/docs/genai/all-about-models/model-providers/local/lm-studio",children:"\ud83d\udcbb LM Studio"})})," - Desktop GUI for model management. User-friendly interface for downloading, managing, and running local models."]}),"\n"]}),"\n",(0,s.jsxs)(o.li,{children:["\n",(0,s.jsxs)(o.p,{children:[(0,s.jsx)(o.strong,{children:(0,s.jsx)(o.a,{href:"/java-ai-resource/docs/genai/all-about-models/model-providers/local/gpt4all",children:"\ud83d\ude80 GPT4All"})})," - Lightweight model runner. Optimized for running smaller models efficiently on consumer hardware."]}),"\n"]}),"\n",(0,s.jsxs)(o.li,{children:["\n",(0,s.jsxs)(o.p,{children:[(0,s.jsx)(o.strong,{children:(0,s.jsx)(o.a,{href:"/java-ai-resource/docs/genai/all-about-models/model-providers/local/in-browser",children:"\ud83c\udf10 In-browser"})})," - Run models directly in your browser. Use WebAssembly and WebGPU to run models without any installation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(o.hr,{}),"\n",(0,s.jsx)(o.p,{children:(0,s.jsx)(o.em,{children:"Written by Lize Raes"})})]})}function h(e={}){const{wrapper:o}={...(0,r.R)(),...e.components};return o?(0,s.jsx)(o,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);